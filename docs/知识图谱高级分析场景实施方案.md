# 知识图谱高级分析场景实施方案

## 文档版本
- **版本**: v1.0
- **创建日期**: 2025-11-17
- **适用图谱**: contract_1117 (v2.0 Schema)

---

## 目录
1. [场景一：FraudRank 欺诈风险传导分析](#场景一)
2. [场景二：高级循环交易检测（分散-汇聚模式）](#场景二)
3. [场景三：空壳公司网络识别](#场景三)
4. [场景四：关联方串通网络分析](#场景四)
5. [技术栈总览](#技术栈)

---

## <a name="场景一"></a>场景一：FraudRank 欺诈风险传导分析

### 1.1 业务背景

在央企合同管理中，风险不是孤立存在的。一家公司涉及法律纠纷，其风险会通过以下路径传导：
- **控股关系**: 母公司风险传导至子公司，反之亦然
- **法人关系**: 法人代表涉案会影响其担任法人的所有公司
- **交易关系**: 供应商/客户的风险会影响交易对手
- **资金流**: 实际资金往来建立的信用依赖关系

**核心问题**：
- 如何量化这种风险传导？
- 哪些看似正常的公司实际上暴露在高风险网络中？
- 如何在签约前进行深度尽职调查？

### 1.2 FraudRank 算法原理

类似 PageRank，但传播的是"欺诈嫌疑分数"而非"权威分数"。

**数学公式**：
```
FraudScore(v) = (1-d) * InitScore(v) + d * Σ(w(u,v) * FraudScore(u) / OutDegree(u))

其中：
- d: 阻尼系数 (建议 0.85)
- InitScore(v): 初始欺诈分数（有法律事件的节点 > 0）
- w(u,v): 边权重（不同关系类型的传导强度）
```

**边权重设计**（基于业务逻辑）：
```python
EDGE_WEIGHTS = {
    'CONTROLS': 0.8,          # 控股关系：强传导
    'LEGAL_PERSON': 0.75,     # 法人关系：强传导
    'PAYS': 0.65,             # 付款关系：中强传导（资金依赖）
    'RECEIVES': 0.60,         # 收款关系：中强传导
    'TRADES_WITH': 0.50,      # 交易关系：中等传导
    'IS_SUPPLIER': 0.45,      # 供应商关系：中弱传导
    'IS_CUSTOMER': 0.40,      # 客户关系：中弱传导
    'PARTY_A': 0.50,          # 合同甲方：中等传导
    'PARTY_B': 0.50,          # 合同乙方：中等传导
}
```

### 1.3 实施步骤

#### 步骤 1: 初始化风险种子节点

从 LegalEvent 出发，识别高风险实体：

**Nebula Graph 查询**：
```sql
-- 1. 找出所有涉及法律事件的公司（通过合同）
USE contract_1117;

MATCH (c:Company)-[:PARTY_A|PARTY_B]->(contract:Contract)-[:RELATED_TO]->(le:LegalEvent)
RETURN c.Company.name AS company_name, 
       c.Company.number AS company_id,
       le.LegalEvent.event_type AS event_type,
       le.LegalEvent.amount AS event_amount,
       le.LegalEvent.status AS event_status;

-- 2. 找出涉及法律事件的人员及其担任法人的公司
MATCH (p:Person)-[:INVOLVED_IN]->(le:LegalEvent),
      (p)-[:LEGAL_PERSON]->(c:Company)
RETURN p.Person.name AS person_name,
       c.Company.name AS company_name,
       c.Company.number AS company_id,
       le.LegalEvent.event_type AS event_type,
       le.LegalEvent.amount AS event_amount;
```

**初始分数计算**：
```python
def calculate_init_score(company_id, legal_events):
    """
    根据涉及的法律事件计算初始风险分数
    
    Args:
        company_id: 公司ID
        legal_events: 该公司涉及的法律事件列表
    
    Returns:
        float: 0-1 之间的初始风险分数
    """
    if not legal_events:
        return 0.0
    
    score = 0.0
    for event in legal_events:
        # 事件类型权重
        type_weight = {
            'Case': 0.8,      # 案件权重高
            'Dispute': 0.5    # 纠纷权重中等
        }.get(event['event_type'], 0.3)
        
        # 金额权重（归一化到 0-1）
        amount_weight = min(event['amount'] / 10000000, 1.0)  # 1000万为上限
        
        # 状态权重
        status_weight = {
            'F': 0.9,  # 已立案
            'I': 0.8,  # 一审
            'J': 0.7,  # 执行
            'N': 0.4   # 已结案
        }.get(event['status'], 0.5)
        
        score += type_weight * amount_weight * status_weight
    
    return min(score, 1.0)  # 上限为 1.0
```

#### 步骤 2: 构建加权图

提取所有关系边并赋予权重：

```python
import os
import csv
from collections import defaultdict

def load_weighted_graph(graph_data_dir):
    """
    从 CSV 文件加载图数据并构建加权邻接表
    
    Returns:
        dict: {
            'nodes': set of node_ids,
            'edges': defaultdict(list),  # node_id -> [(neighbor_id, weight), ...]
            'out_degree': defaultdict(int)
        }
    """
    EDGE_WEIGHTS = {
        'CONTROLS': 0.8,
        'LEGAL_PERSON': 0.75,
        'PAYS': 0.65,
        'RECEIVES': 0.60,
        'TRADES_WITH': 0.50,
        'IS_SUPPLIER': 0.45,
        'IS_CUSTOMER': 0.40,
        'PARTY_A': 0.50,
        'PARTY_B': 0.50,
    }
    
    graph = {
        'nodes': set(),
        'edges': defaultdict(list),
        'out_degree': defaultdict(int)
    }
    
    # 读取所有边文件
    edge_files = {
        'edges_controls.csv': 'CONTROLS',
        'edges_legal_person.csv': 'LEGAL_PERSON',
        'edges_company_transaction.csv': None,  # 需要解析边类型
        'edges_trades_with.csv': 'TRADES_WITH',
        'edges_is_supplier.csv': 'IS_SUPPLIER',
        'edges_is_customer.csv': 'IS_CUSTOMER',
    }
    
    for filename, edge_type in edge_files.items():
        filepath = os.path.join(graph_data_dir, filename)
        if not os.path.exists(filepath):
            continue
            
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                from_node = row['from_node']
                to_node = row['to_node']
                
                # 对于 company_transaction，需要从边类型字段读取
                if filename == 'edges_company_transaction.csv':
                    edge_type = row['edge_type']
                
                weight = EDGE_WEIGHTS.get(edge_type, 0.3)
                
                graph['nodes'].add(from_node)
                graph['nodes'].add(to_node)
                graph['edges'][from_node].append((to_node, weight))
                graph['out_degree'][from_node] += 1
    
    return graph
```

#### 步骤 3: 迭代计算 FraudRank

```python
def compute_fraud_rank(graph, init_scores, damping=0.85, max_iter=100, tolerance=1e-6):
    """
    计算 FraudRank 分数
    
    Args:
        graph: 图数据结构
        init_scores: dict {node_id: init_score}
        damping: 阻尼系数
        max_iter: 最大迭代次数
        tolerance: 收敛阈值
    
    Returns:
        dict: {node_id: fraud_rank_score}
    """
    # 初始化所有节点分数
    scores = {node: init_scores.get(node, 0.0) for node in graph['nodes']}
    
    for iteration in range(max_iter):
        new_scores = {}
        max_diff = 0.0
        
        for node in graph['nodes']:
            # 基础分数（保留初始风险）
            base_score = (1 - damping) * init_scores.get(node, 0.0)
            
            # 从入边传播来的分数
            propagated_score = 0.0
            for neighbor, neighbors_list in graph['edges'].items():
                for target, weight in neighbors_list:
                    if target == node:
                        # neighbor -> node 的边
                        out_deg = graph['out_degree'][neighbor]
                        if out_deg > 0:
                            propagated_score += weight * scores[neighbor] / out_deg
            
            new_scores[node] = base_score + damping * propagated_score
            max_diff = max(max_diff, abs(new_scores[node] - scores[node]))
        
        scores = new_scores
        
        if max_diff < tolerance:
            print(f"收敛于第 {iteration + 1} 次迭代")
            break
    
    return scores
```

#### 步骤 4: 结果分析与可视化

```python
def analyze_fraud_rank_results(fraud_scores, graph_data_dir, top_n=50):
    """
    分析 FraudRank 结果并生成报告
    """
    import pandas as pd
    
    # 加载公司信息
    companies_df = pd.read_csv(
        os.path.join(graph_data_dir, 'nodes_company.csv')
    )
    company_info = companies_df.set_index('node_id').to_dict('index')
    
    # 按分数排序
    sorted_scores = sorted(
        fraud_scores.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    # 生成报告
    report = []
    for node_id, score in sorted_scores[:top_n]:
        if node_id.startswith('ORG_') or node_id.startswith('SUP_') or node_id.startswith('CUS_'):
            info = company_info.get(node_id, {})
            report.append({
                '公司ID': node_id,
                '公司名称': info.get('name', 'Unknown'),
                '风险分数': round(score, 4),
                '风险等级': get_risk_level(score),
                '法人代表': info.get('legal_person', 'N/A'),
                '信用代码': info.get('credit_code', 'N/A')
            })
    
    df_report = pd.DataFrame(report)
    df_report.to_csv('fraud_rank_report.csv', index=False, encoding='utf-8-sig')
    
    return df_report

def get_risk_level(score):
    """风险等级划分"""
    if score >= 0.7:
        return '高风险'
    elif score >= 0.4:
        return '中风险'
    elif score >= 0.2:
        return '低风险'
    else:
        return '正常'
```

#### 步骤 5: 完整实施脚本

```python
# fraud_rank_analysis.py

import os
import sys
from collections import defaultdict

# 配置路径
GRAPH_DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'graph_data')

def main():
    print("=" * 60)
    print("FraudRank 欺诈风险传导分析")
    print("=" * 60)
    
    # Step 1: 加载图数据
    print("\n[1/4] 加载图数据...")
    graph = load_weighted_graph(GRAPH_DATA_DIR)
    print(f"  节点数: {len(graph['nodes'])}")
    print(f"  边数: {sum(len(v) for v in graph['edges'].values())}")
    
    # Step 2: 初始化风险种子
    print("\n[2/4] 初始化风险种子节点...")
    init_scores = initialize_risk_seeds(GRAPH_DATA_DIR)
    print(f"  风险种子节点数: {sum(1 for s in init_scores.values() if s > 0)}")
    
    # Step 3: 计算 FraudRank
    print("\n[3/4] 计算 FraudRank（迭代中...）")
    fraud_scores = compute_fraud_rank(graph, init_scores, damping=0.85)
    
    # Step 4: 生成分析报告
    print("\n[4/4] 生成分析报告...")
    report = analyze_fraud_rank_results(fraud_scores, GRAPH_DATA_DIR, top_n=50)
    
    print("\n" + "=" * 60)
    print("分析完成！")
    print("=" * 60)
    print(f"\n前 10 高风险公司：\n")
    print(report.head(10).to_string(index=False))
    print(f"\n完整报告已保存至: fraud_rank_report.csv")

def initialize_risk_seeds(graph_data_dir):
    """
    从法律事件数据初始化风险种子
    """
    import csv
    
    init_scores = defaultdict(float)
    
    # 从案件-合同关系推导公司风险
    edges_case_contract = os.path.join(graph_data_dir, 'edges_case_contract.csv')
    edges_party = os.path.join(graph_data_dir, 'edges_party.csv')
    nodes_legal_event = os.path.join(graph_data_dir, 'nodes_legal_event.csv')
    
    # 加载法律事件信息
    legal_events = {}
    with open(nodes_legal_event, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            legal_events[row['node_id']] = {
                'event_type': row['event_type'],
                'amount': float(row['amount']) if row['amount'] else 0,
                'status': row['status']
            }
    
    # 合同 -> 法律事件映射
    contract_to_event = {}
    with open(edges_case_contract, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            contract_to_event[row['from_node']] = row['to_node']
    
    # 合同 -> 公司映射（甲方/乙方）
    contract_to_companies = defaultdict(list)
    with open(edges_party, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            contract_to_companies[row['to_node']].append(row['from_node'])
    
    # 计算初始分数
    for contract_id, event_id in contract_to_event.items():
        if event_id in legal_events:
            event = legal_events[event_id]
            base_score = calculate_init_score(None, [event])
            
            # 将风险分配给该合同的甲乙方
            for company_id in contract_to_companies.get(contract_id, []):
                init_scores[company_id] = max(init_scores[company_id], base_score)
    
    return dict(init_scores)

if __name__ == '__main__':
    main()
```

### 1.4 预期输出

**CSV 报告示例**：
```csv
公司ID,公司名称,风险分数,风险等级,法人代表,信用代码
ORG_015,中天建筑材料,0.8234,高风险,王芳,91110000MA012345XX
SUP_008,华信建材有限公司,0.7156,高风险,李强,91110000MA012346XX
ORG_003,中建华南分公司,0.4521,中风险,张伟,91110000MA012347XX
...
```

**可视化分析**（可选）：
- 使用 Gephi 或 Cytoscape 导入节点和边，按 FraudRank 分数着色
- 高风险节点用红色，正常节点用绿色
- 观察风险传导路径

### 1.5 业务应用

1. **合同审批流程**：
   - 风险分数 >= 0.7：需集团高层审批 + 现场尽调
   - 风险分数 0.4-0.7：需部门领导审批 + 额外担保
   - 风险分数 < 0.4：常规流程

2. **供应商准入评估**：
   - 新供应商准入前必须查询 FraudRank
   - 高风险供应商禁止准入或限额合作

3. **定期风险扫描**：
   - 每月重新计算一次（法律事件数据更新后）
   - 监控现有合作伙伴的风险变化趋势

---

## <a name="场景二"></a>场景二：高级循环交易检测（分散-汇聚模式）

### 2.1 业务背景

传统的循环交易检测只能发现简单的环形结构 `A→B→C→A`。但实际中，企业为了逃避监管，会采用更复杂的模式：

**分散-汇聚模式**：
```
          ┌─→ B1 ─┐
Central A ├─→ B2 ─┤
          └─→ B3 ─┘
              ↓
         (B1,B2,B3 互相交易)
              ↓
          最终汇聚回 A 或 A的关联公司
```

**特征**：
- 资金从核心公司快速分散到多个"中转站"
- 中转站之间形成复杂交易网络
- 最终资金以不同路径汇聚回核心公司或其关联方
- 时间窗口：通常在 3-6 个月内完成

### 2.2 检测策略

#### 策略 1: 扇出-扇入分析

```python
def detect_fan_out_fan_in(graph_data_dir, time_window_days=180, amount_threshold=1000000):
    """
    检测分散-汇聚模式的循环交易
    
    Args:
        graph_data_dir: 图数据目录
        time_window_days: 时间窗口（天）
        amount_threshold: 金额阈值
    
    Returns:
        list: 可疑模式列表
    """
    import pandas as pd
    from datetime import datetime, timedelta
    
    # 加载交易数据
    transactions_df = pd.read_csv(
        os.path.join(graph_data_dir, 'nodes_transaction.csv')
    )
    transactions_df['transaction_date'] = pd.to_datetime(
        transactions_df['transaction_date']
    )
    
    # 加载公司-交易关系
    company_txn_edges = pd.read_csv(
        os.path.join(graph_data_dir, 'edges_company_transaction.csv')
    )
    
    # 构建交易图：公司 -> 交易 -> 公司
    # PAYS: company -> transaction (付款方)
    # RECEIVES: transaction -> company (收款方)
    
    pays_edges = company_txn_edges[company_txn_edges['edge_type'] == 'PAYS']
    receives_edges = company_txn_edges[company_txn_edges['edge_type'] == 'RECEIVES']
    
    # 合并得到：付款公司 -> 收款公司
    money_flows = pays_edges.merge(
        receives_edges,
        left_on='to_node',  # transaction_id
        right_on='from_node',
        suffixes=('_payer', '_receiver')
    )
    
    # 添加交易信息
    money_flows = money_flows.merge(
        transactions_df[['node_id', 'amount', 'transaction_date', 'transaction_type']],
        left_on='to_node_payer',
        right_on='node_id'
    )
    
    # 重命名列
    money_flows.rename(columns={
        'from_node_payer': 'payer_company',
        'to_node_receiver': 'receiver_company',
        'amount': 'transaction_amount',
        'transaction_date': 'txn_date'
    }, inplace=True)
    
    suspicious_patterns = []
    
    # 对每个公司作为潜在的"核心公司"
    for central_company in money_flows['payer_company'].unique():
        # Step 1: 找出从该公司流出的所有交易
        outflows = money_flows[
            (money_flows['payer_company'] == central_company) &
            (money_flows['transaction_amount'] >= amount_threshold)
        ]
        
        if len(outflows) < 2:  # 至少分散到 2 个公司
            continue
        
        # 获取时间范围
        min_date = outflows['txn_date'].min()
        max_date = min_date + timedelta(days=time_window_days)
        
        # Step 2: 找出在时间窗口内流出的目标公司（分散节点）
        dispersed_companies = set(outflows['receiver_company'].unique())
        total_outflow = outflows['transaction_amount'].sum()
        
        # Step 3: 检查这些分散节点之间是否有交易
        inter_trades = money_flows[
            (money_flows['payer_company'].isin(dispersed_companies)) &
            (money_flows['receiver_company'].isin(dispersed_companies)) &
            (money_flows['txn_date'] >= min_date) &
            (money_flows['txn_date'] <= max_date)
        ]
        
        # Step 4: 检查是否有资金汇聚回核心公司或其关联公司
        related_companies = get_related_companies(central_company, graph_data_dir)
        
        inflows = money_flows[
            (money_flows['receiver_company'].isin(related_companies)) &
            (money_flows['payer_company'].isin(dispersed_companies)) &
            (money_flows['txn_date'] >= min_date) &
            (money_flows['txn_date'] <= max_date)
        ]
        
        if len(inflows) > 0:
            total_inflow = inflows['transaction_amount'].sum()
            
            # 计算相似度
            similarity = min(total_inflow, total_outflow) / max(total_inflow, total_outflow)
            
            if similarity >= 0.7:  # 流入流出金额相似度 >= 70%
                suspicious_patterns.append({
                    'central_company': central_company,
                    'dispersed_companies': list(dispersed_companies),
                    'related_companies': list(related_companies),
                    'total_outflow': total_outflow,
                    'total_inflow': total_inflow,
                    'similarity': similarity,
                    'inter_trade_count': len(inter_trades),
                    'time_span_days': (inflows['txn_date'].max() - min_date).days,
                    'risk_score': calculate_circular_trade_risk(
                        similarity, len(dispersed_companies), len(inter_trades)
                    )
                })
    
    return suspicious_patterns

def get_related_companies(company_id, graph_data_dir):
    """
    获取公司的关联方：
    1. 共同法人的公司
    2. 控股/被控股的公司
    """
    import csv
    
    related = {company_id}
    
    # 1. 通过法人关系
    legal_person_edges = os.path.join(graph_data_dir, 'edges_legal_person.csv')
    controls_edges = os.path.join(graph_data_dir, 'edges_controls.csv')
    
    # 找到该公司的法人
    company_to_person = {}
    person_to_companies = {}
    
    with open(legal_person_edges, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            person_id = row['from_node']
            comp_id = row['to_node']
            company_to_person[comp_id] = person_id
            if person_id not in person_to_companies:
                person_to_companies[person_id] = []
            person_to_companies[person_id].append(comp_id)
    
    if company_id in company_to_person:
        legal_person = company_to_person[company_id]
        related.update(person_to_companies.get(legal_person, []))
    
    # 2. 通过控股关系
    with open(controls_edges, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            parent = row['from_node']
            subsidiary = row['to_node']
            if parent == company_id:
                related.add(subsidiary)
            if subsidiary == company_id:
                related.add(parent)
    
    return related

def calculate_circular_trade_risk(similarity, num_dispersed, num_inter_trades):
    """
    计算循环交易风险分数
    """
    # 金额相似度权重 40%
    similarity_score = similarity * 0.4
    
    # 分散节点数量权重 30% (越多越可疑)
    dispersed_score = min(num_dispersed / 10, 1.0) * 0.3
    
    # 中间交易密度权重 30%
    inter_trade_score = min(num_inter_trades / 20, 1.0) * 0.3
    
    return similarity_score + dispersed_score + inter_trade_score
```

#### 策略 2: 基于 Nebula Graph 的路径查询

```sql
-- 在 Nebula Graph 中查询分散-汇聚模式

USE contract_1117;

-- 1. 找出某公司的多跳资金流出-流入路径
MATCH p = (central:Company {number: "ORG_001"})-[:PAYS|RECEIVES*2..8]-(central)
WHERE length(p) >= 4
RETURN p, length(p) AS path_length, 
       [n IN nodes(p) | n.Company.name] AS company_path
ORDER BY path_length
LIMIT 50;

-- 2. 找出分散节点之间的密集交易
MATCH (central:Company {number: "ORG_001"})-[:PAYS]->(t1:Transaction)-[:RECEIVES]->(dispersed:Company),
      (dispersed)-[:PAYS|RECEIVES*1..4]-(other_dispersed:Company)-[:PAYS]->(t2:Transaction)-[:RECEIVES]->(related:Company)
WHERE related IN (central)-[:LEGAL_PERSON|CONTROLS*0..2]-(:Company)
RETURN central.Company.name AS 核心公司,
       collect(DISTINCT dispersed.Company.name) AS 分散节点,
       collect(DISTINCT related.Company.name) AS 汇聚节点,
       sum(t1.Transaction.amount) AS 流出金额,
       sum(t2.Transaction.amount) AS 流入金额;

-- 3. 检测具有共同法人的循环交易
MATCH (c1:Company)-[:PAYS|RECEIVES*3..6]-(c2:Company),
      (c1)<-[:LEGAL_PERSON]-(p:Person)-[:LEGAL_PERSON]->(c2)
WHERE c1 <> c2
RETURN c1.Company.name, c2.Company.name, p.Person.name AS 共同法人;
```

### 2.3 完整实施脚本

```python
# advanced_circular_trade_detection.py

import os
import pandas as pd
from datetime import datetime

GRAPH_DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'graph_data')

def main():
    print("=" * 70)
    print("高级循环交易检测 - 分散汇聚模式分析")
    print("=" * 70)
    
    print("\n[1/3] 分析资金流向...")
    suspicious_patterns = detect_fan_out_fan_in(
        GRAPH_DATA_DIR,
        time_window_days=180,
        amount_threshold=500000  # 50万以上
    )
    
    print(f"  发现可疑模式数: {len(suspicious_patterns)}")
    
    print("\n[2/3] 生成详细报告...")
    report_df = pd.DataFrame(suspicious_patterns)
    
    if len(report_df) > 0:
        # 按风险分数排序
        report_df = report_df.sort_values('risk_score', ascending=False)
        
        # 保存报告
        report_df.to_csv(
            'circular_trade_detection_report.csv',
            index=False,
            encoding='utf-8-sig'
        )
        
        print("\n[3/3] 前 5 高风险循环交易模式：\n")
        for idx, row in report_df.head(5).iterrows():
            print(f"模式 #{idx + 1}")
            print(f"  核心公司: {row['central_company']}")
            print(f"  分散节点: {', '.join(row['dispersed_companies'][:3])}... ({len(row['dispersed_companies'])} 个)")
            print(f"  流出金额: ¥{row['total_outflow']:,.2f}")
            print(f"  流入金额: ¥{row['total_inflow']:,.2f}")
            print(f"  相似度: {row['similarity']:.2%}")
            print(f"  风险分数: {row['risk_score']:.4f}")
            print(f"  时间跨度: {row['time_span_days']} 天")
            print()
        
        print(f"完整报告已保存至: circular_trade_detection_report.csv")
    else:
        print("\n未发现可疑的循环交易模式")

if __name__ == '__main__':
    main()
```

### 2.4 预期输出

```
高级循环交易检测 - 分散汇聚模式分析
======================================================================

[1/3] 分析资金流向...
  发现可疑模式数: 3

[2/3] 生成详细报告...

[3/3] 前 5 高风险循环交易模式：

模式 #1
  核心公司: ORG_008
  分散节点: SUP_012, SUP_015, CUS_003... (4 个)
  流出金额: ¥12,500,000.00
  流入金额: ¥11,800,000.00
  相似度: 94.40%
  风险分数: 0.7856
  时间跨度: 127 天

模式 #2
  核心公司: ORG_015
  分散节点: SUP_008, ORG_022... (3 个)
  流出金额: ¥8,300,000.00
  流入金额: ¥7,950,000.00
  相似度: 95.78%
  风险分数: 0.7234
  时间跨度: 89 天

完整报告已保存至: circular_trade_detection_report.csv
```

---

## <a name="场景三"></a>场景三：空壳公司网络识别

### 3.1 业务背景

空壳公司（Shell Company）在合同欺诈中扮演重要角色：
- 虚增交易额
- 资金中转洗钱
- 虚开发票
- 规避税务监管

**典型特征**：
- 资金快速进出（pass-through）
- 交易对手单一
- 合同金额与公司规模不匹配
- 多个空壳公司共享法人或控制人

### 3.2 检测指标体系

```python
def extract_shell_company_features(company_id, graph_data_dir):
    """
    提取空壳公司特征
    
    Returns:
        dict: 特征字典
    """
    import pandas as pd
    import numpy as np
    
    # 加载数据
    transactions_df = pd.read_csv(os.path.join(graph_data_dir, 'nodes_transaction.csv'))
    company_txn_edges = pd.read_csv(os.path.join(graph_data_dir, 'edges_company_transaction.csv'))
    contracts_df = pd.read_csv(os.path.join(graph_data_dir, 'nodes_contract.csv'))
    
    # 特征 1: 资金穿透率 (Pass-through Ratio)
    pays_txns = company_txn_edges[
        (company_txn_edges['from_node'] == company_id) &
        (company_txn_edges['edge_type'] == 'PAYS')
    ]['to_node'].tolist()
    
    receives_txns = company_txn_edges[
        (company_txn_edges['to_node'] == company_id) &
        (company_txn_edges['edge_type'] == 'RECEIVES')
    ]['from_node'].tolist()
    
    total_outflow = transactions_df[
        transactions_df['node_id'].isin(pays_txns)
    ]['amount'].sum()
    
    total_inflow = transactions_df[
        transactions_df['node_id'].isin(receives_txns)
    ]['amount'].sum()
    
    pass_through_ratio = min(total_inflow, total_outflow) / max(total_inflow, total_outflow) if max(total_inflow, total_outflow) > 0 else 0
    
    # 特征 2: 交易速度 (Transaction Velocity)
    transactions_df['transaction_date'] = pd.to_datetime(transactions_df['transaction_date'])
    
    txn_dates = transactions_df[
        transactions_df['node_id'].isin(pays_txns + receives_txns)
    ]['transaction_date'].sort_values()
    
    if len(txn_dates) > 1:
        avg_time_gap = (txn_dates.max() - txn_dates.min()).days / (len(txn_dates) - 1)
    else:
        avg_time_gap = 0
    
    # 特征 3: 交易对手多样性 (Partner Diversity)
    all_partners = set()
    pays_edges = company_txn_edges[
        (company_txn_edges['from_node'] == company_id) &
        (company_txn_edges['edge_type'] == 'PAYS')
    ]
    
    for txn_id in pays_edges['to_node']:
        # 找到这笔交易的收款方
        receiver = company_txn_edges[
            (company_txn_edges['from_node'] == txn_id) &
            (company_txn_edges['edge_type'] == 'RECEIVES')
        ]['to_node'].tolist()
        all_partners.update(receiver)
    
    receives_edges = company_txn_edges[
        (company_txn_edges['to_node'] == company_id) &
        (company_txn_edges['edge_type'] == 'RECEIVES')
    ]
    
    for txn_id in receives_edges['from_node']:
        # 找到这笔交易的付款方
        payer = company_txn_edges[
            (company_txn_edges['to_node'] == txn_id) &
            (company_txn_edges['edge_type'] == 'PAYS')
        ]['from_node'].tolist()
        all_partners.update(payer)
    
    total_txns = len(pays_txns) + len(receives_txns)
    partner_diversity = len(all_partners) / total_txns if total_txns > 0 else 0
    
    # 特征 4: 合同集中度 (Contract Concentration)
    # 该公司作为甲方或乙方的合同
    party_edges = pd.read_csv(os.path.join(graph_data_dir, 'edges_party.csv'))
    company_contracts = party_edges[
        party_edges['from_node'] == company_id
    ]['to_node'].unique()
    
    # 特征 5: 网络中心性 (Network Centrality)
    # 简化版：度中心性
    degree = len(all_partners)
    
    # 特征 6: 法人公司数量 (Legal Person Company Count)
    legal_person_count = count_companies_with_same_legal_person(company_id, graph_data_dir)
    
    return {
        'company_id': company_id,
        'pass_through_ratio': pass_through_ratio,
        'transaction_velocity_days': avg_time_gap,
        'partner_diversity': partner_diversity,
        'total_transaction_count': total_txns,
        'total_inflow': total_inflow,
        'total_outflow': total_outflow,
        'degree_centrality': degree,
        'legal_person_company_count': legal_person_count,
        'contract_count': len(company_contracts)
    }

def count_companies_with_same_legal_person(company_id, graph_data_dir):
    """统计与该公司共享法人的公司数量"""
    import csv
    
    legal_person_edges = os.path.join(graph_data_dir, 'edges_legal_person.csv')
    
    company_to_person = {}
    person_to_companies = {}
    
    with open(legal_person_edges, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            person_id = row['from_node']
            comp_id = row['to_node']
            company_to_person[comp_id] = person_id
            if person_id not in person_to_companies:
                person_to_companies[person_id] = []
            person_to_companies[person_id].append(comp_id)
    
    if company_id in company_to_person:
        legal_person = company_to_person[company_id]
        return len(person_to_companies[legal_person]) - 1  # 不包括自己
    
    return 0

def calculate_shell_company_score(features):
    """
    计算空壳公司嫌疑分数 (0-1)
    """
    score = 0.0
    
    # 1. 穿透率高 (0.8-1.0) => 高嫌疑
    if features['pass_through_ratio'] >= 0.9:
        score += 0.25
    elif features['pass_through_ratio'] >= 0.8:
        score += 0.15
    
    # 2. 交易速度快 (< 7 天) => 高嫌疑
    if 0 < features['transaction_velocity_days'] < 7:
        score += 0.20
    elif 7 <= features['transaction_velocity_days'] < 30:
        score += 0.10
    
    # 3. 交易对手单一 (diversity < 0.3) => 高嫌疑
    if features['partner_diversity'] < 0.2:
        score += 0.20
    elif features['partner_diversity'] < 0.4:
        score += 0.10
    
    # 4. 法人关联公司多 (>= 5) => 高嫌疑
    if features['legal_person_company_count'] >= 5:
        score += 0.20
    elif features['legal_person_company_count'] >= 3:
        score += 0.10
    
    # 5. 合同数量少但金额大 => 高嫌疑
    if features['contract_count'] > 0:
        avg_contract_amount = (features['total_inflow'] + features['total_outflow']) / features['contract_count']
        if avg_contract_amount > 5000000 and features['contract_count'] < 3:
            score += 0.15
    
    return min(score, 1.0)
```

### 3.3 完整实施脚本

```python
# shell_company_detection.py

import os
import pandas as pd
from tqdm import tqdm

GRAPH_DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'graph_data')

def main():
    print("=" * 70)
    print("空壳公司网络识别分析")
    print("=" * 70)
    
    # 加载所有公司
    companies_df = pd.read_csv(os.path.join(GRAPH_DATA_DIR, 'nodes_company.csv'))
    
    print(f"\n[1/3] 提取特征 (共 {len(companies_df)} 家公司)...")
    
    all_features = []
    for company_id in tqdm(companies_df['node_id']):
        features = extract_shell_company_features(company_id, GRAPH_DATA_DIR)
        features['shell_score'] = calculate_shell_company_score(features)
        
        # 添加公司基本信息
        company_info = companies_df[companies_df['node_id'] == company_id].iloc[0]
        features['company_name'] = company_info['name']
        features['legal_person'] = company_info['legal_person']
        
        all_features.append(features)
    
    print("\n[2/3] 分析结果...")
    features_df = pd.DataFrame(all_features)
    features_df = features_df.sort_values('shell_score', ascending=False)
    
    # 筛选高嫌疑公司
    high_risk = features_df[features_df['shell_score'] >= 0.6]
    
    print(f"  高嫌疑空壳公司数量: {len(high_risk)} ({len(high_risk)/len(features_df)*100:.1f}%)")
    
    print("\n[3/3] 生成报告...")
    features_df.to_csv(
        'shell_company_detection_report.csv',
        index=False,
        encoding='utf-8-sig'
    )
    
    print("\n前 10 高嫌疑空壳公司：\n")
    print(high_risk.head(10)[[
        'company_id', 'company_name', 'shell_score',
        'pass_through_ratio', 'partner_diversity', 'legal_person_company_count'
    ]].to_string(index=False))
    
    print(f"\n完整报告已保存至: shell_company_detection_report.csv")
    
    # 额外分析：识别空壳公司网络
    print("\n[额外] 识别空壳公司网络（共享法人）...")
    shell_networks = identify_shell_networks(high_risk, GRAPH_DATA_DIR)
    
    if shell_networks:
        print(f"  发现 {len(shell_networks)} 个空壳公司网络")
        for i, network in enumerate(shell_networks[:3], 1):
            print(f"\n  网络 #{i}:")
            print(f"    共同法人: {network['legal_person']}")
            print(f"    公司数量: {len(network['companies'])}")
            print(f"    公司列表: {', '.join(network['companies'][:5])}...")

def identify_shell_networks(high_risk_df, graph_data_dir):
    """识别共享法人的空壳公司网络"""
    import csv
    from collections import defaultdict
    
    legal_person_edges = os.path.join(graph_data_dir, 'edges_legal_person.csv')
    person_to_companies = defaultdict(list)
    
    with open(legal_person_edges, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            person_id = row['from_node']
            comp_id = row['to_node']
            person_to_companies[person_id].append(comp_id)
    
    # 筛选出至少有 2 家高嫌疑公司的法人
    networks = []
    for person_id, companies in person_to_companies.items():
        high_risk_companies = [
            c for c in companies 
            if c in high_risk_df['company_id'].values
        ]
        
        if len(high_risk_companies) >= 2:
            # 加载人员信息
            persons_df = pd.read_csv(os.path.join(graph_data_dir, 'nodes_person.csv'))
            person_info = persons_df[persons_df['node_id'] == person_id]
            
            person_name = person_info.iloc[0]['name'] if len(person_info) > 0 else person_id
            
            networks.append({
                'legal_person': person_name,
                'person_id': person_id,
                'companies': high_risk_companies,
                'network_size': len(high_risk_companies)
            })
    
    return sorted(networks, key=lambda x: x['network_size'], reverse=True)

if __name__ == '__main__':
    main()
```

### 3.4 预期输出

```
空壳公司网络识别分析
======================================================================

[1/3] 提取特征 (共 92 家公司)...
100%|████████████████████████████████████████████| 92/92 [00:02<00:00, 38.5it/s]

[2/3] 分析结果...
  高嫌疑空壳公司数量: 12 (13.0%)

[3/3] 生成报告...

前 10 高嫌疑空壳公司：

company_id  company_name          shell_score  pass_through_ratio  partner_diversity  legal_person_company_count
SUP_015     东方物流配送           0.85         0.96                0.15               6
CUS_008     华通贸易有限公司       0.75         0.89                0.22               4
SUP_022     鑫达供应链            0.70         0.93                0.18               5
...

完整报告已保存至: shell_company_detection_report.csv

[额外] 识别空壳公司网络（共享法人）...
  发现 3 个空壳公司网络

  网络 #1:
    共同法人: 王强
    公司数量: 6
    公司列表: SUP_015, SUP_018, CUS_008, SUP_022, CUS_011...

  网络 #2:
    共同法人: 李娜
    公司数量: 4
    公司列表: SUP_025, CUS_015, SUP_028, CUS_019...
```

---

## <a name="场景四"></a>场景四：关联方串通网络分析

### 4.1 业务背景

在招投标和合同审批中，关联方串通（Collusion）会导致：
- 围标/陪标
- 虚假竞争
- 利益输送
- 价格操纵

**典型模式**：
- 多家公司共享法人代表
- 存在控股关系但对外装作独立公司
- 轮流中标（Win Rotation）
- 合同金额刻意设计在审批阈值以下

### 4.2 检测算法

```python
def detect_collusion_network(graph_data_dir, min_cluster_size=3):
    """
    检测关联方串通网络
    
    Returns:
        list: 可疑串通网络列表
    """
    import networkx as nx
    import csv
    from collections import defaultdict
    
    # 构建关联关系图
    G = nx.Graph()
    
    # 1. 加载所有公司节点
    companies_df = pd.read_csv(os.path.join(graph_data_dir, 'nodes_company.csv'))
    for _, row in companies_df.iterrows():
        G.add_node(row['node_id'], name=row['name'], type='company')
    
    # 2. 添加共享法人的边
    legal_person_edges = os.path.join(graph_data_dir, 'edges_legal_person.csv')
    person_to_companies = defaultdict(list)
    
    with open(legal_person_edges, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            person_to_companies[row['from_node']].append(row['to_node'])
    
    for person_id, companies in person_to_companies.items():
        if len(companies) >= 2:
            # 这些公司之间建立"共享法人"边
            for i in range(len(companies)):
                for j in range(i + 1, len(companies)):
                    G.add_edge(
                        companies[i], companies[j],
                        relation_type='shared_legal_person',
                        person_id=person_id,
                        weight=1.0
                    )
    
    # 3. 添加控股关系的边
    controls_edges = os.path.join(graph_data_dir, 'edges_controls.csv')
    with open(controls_edges, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if G.has_node(row['from_node']) and G.has_node(row['to_node']):
                G.add_edge(
                    row['from_node'], row['to_node'],
                    relation_type='controls',
                    weight=0.9
                )
    
    # 4. 社区检测：找出紧密连接的公司集群
    from networkx.algorithms import community
    
    communities = list(community.greedy_modularity_communities(G, weight='weight'))
    
    suspicious_networks = []
    
    for comm_idx, comm in enumerate(communities):
        if len(comm) < min_cluster_size:
            continue
        
        comm_list = list(comm)
        
        # 分析这个集群的可疑行为
        collusion_features = analyze_collusion_patterns(
            comm_list, graph_data_dir, G
        )
        
        if collusion_features['risk_score'] >= 0.5:
            suspicious_networks.append({
                'network_id': f"NETWORK_{comm_idx + 1}",
                'companies': comm_list,
                'size': len(comm_list),
                **collusion_features
            })
    
    return suspicious_networks

def analyze_collusion_patterns(company_cluster, graph_data_dir, relation_graph):
    """
    分析公司集群的串通模式
    """
    import pandas as pd
    import numpy as np
    
    # 加载合同数据
    contracts_df = pd.read_csv(os.path.join(graph_data_dir, 'nodes_contract.csv'))
    party_edges_df = pd.read_csv(os.path.join(graph_data_dir, 'edges_party.csv'))
    
    # 特征 1: 轮流中标模式检测
    # 找出集群内公司作为乙方的合同
    cluster_contracts = party_edges_df[
        (party_edges_df['from_node'].isin(company_cluster)) &
        (party_edges_df['edge_type'] == 'PARTY_B')
    ]
    
    if len(cluster_contracts) == 0:
        return {'risk_score': 0.0}
    
    # 按签订日期排序
    cluster_contracts = cluster_contracts.merge(
        contracts_df[['node_id', 'sign_date', 'amount']],
        left_on='to_node',
        right_on='node_id'
    )
    cluster_contracts['sign_date'] = pd.to_datetime(cluster_contracts['sign_date'])
    cluster_contracts = cluster_contracts.sort_values('sign_date')
    
    # 计算中标轮换度
    win_companies = cluster_contracts['from_node'].tolist()
    rotation_score = calculate_rotation_score(win_companies)
    
    # 特征 2: 合同金额相似度
    amounts = cluster_contracts['amount'].dropna()
    if len(amounts) >= 2:
        amount_std = amounts.std()
        amount_mean = amounts.mean()
        amount_cv = amount_std / amount_mean if amount_mean > 0 else 0
        amount_similarity = 1 - min(amount_cv, 1.0)  # CV 越小，相似度越高
    else:
        amount_similarity = 0
    
    # 特征 3: 合同金额卡阈值检测
    threshold_count = sum(1 for amt in amounts if is_near_threshold(amt))
    threshold_ratio = threshold_count / len(amounts) if len(amounts) > 0 else 0
    
    # 特征 4: 网络密度（关联关系的紧密程度）
    subgraph = relation_graph.subgraph(company_cluster)
    density = nx.density(subgraph)
    
    # 特征 5: 关联类型强度
    relation_types = []
    for u, v, data in subgraph.edges(data=True):
        relation_types.append(data.get('relation_type'))
    
    has_strong_relation = any(
        rt in ['shared_legal_person', 'controls'] 
        for rt in relation_types
    )
    
    # 综合风险分数
    risk_score = (
        rotation_score * 0.3 +
        amount_similarity * 0.2 +
        threshold_ratio * 0.2 +
        density * 0.2 +
        (0.1 if has_strong_relation else 0)
    )
    
    return {
        'risk_score': risk_score,
        'rotation_score': rotation_score,
        'amount_similarity': amount_similarity,
        'threshold_ratio': threshold_ratio,
        'network_density': density,
        'contract_count': len(cluster_contracts),
        'total_amount': amounts.sum(),
        'avg_amount': amounts.mean()
    }

def calculate_rotation_score(win_sequence):
    """
    计算轮换分数：检测是否存在规律的轮流中标
    完美轮换 = 1.0，完全随机 = 0.0
    """
    if len(win_sequence) < 3:
        return 0.0
    
    from collections import Counter
    
    # 统计每个公司出现的次数
    counter = Counter(win_sequence)
    
    # 方差越小，说明分布越均匀（越像轮换）
    counts = list(counter.values())
    if len(counts) < 2:
        return 0.0
    
    mean_count = sum(counts) / len(counts)
    variance = sum((c - mean_count) ** 2 for c in counts) / len(counts)
    
    # 归一化：方差为 0 时分数为 1
    max_variance = mean_count ** 2
    rotation_score = 1 - min(variance / max_variance, 1.0) if max_variance > 0 else 0
    
    return rotation_score

def is_near_threshold(amount, thresholds=[1000000, 3000000, 5000000, 10000000], margin=0.05):
    """
    检测金额是否刻意卡在审批阈值附近
    """
    for threshold in thresholds:
        lower = threshold * (1 - margin)
        upper = threshold
        if lower <= amount <= upper:
            return True
    return False
```

### 4.3 完整实施脚本

```python
# collusion_network_detection.py

import os
import pandas as pd
import networkx as nx

GRAPH_DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'graph_data')

def main():
    print("=" * 70)
    print("关联方串通网络分析")
    print("=" * 70)
    
    print("\n[1/3] 构建关联关系图...")
    suspicious_networks = detect_collusion_network(GRAPH_DATA_DIR, min_cluster_size=3)
    
    print(f"  发现可疑串通网络数: {len(suspicious_networks)}")
    
    if len(suspicious_networks) == 0:
        print("\n未发现可疑的串通网络")
        return
    
    print("\n[2/3] 分析串通模式...")
    
    # 加载公司信息用于展示
    companies_df = pd.read_csv(os.path.join(GRAPH_DATA_DIR, 'nodes_company.csv'))
    company_names = companies_df.set_index('node_id')['name'].to_dict()
    
    # 生成详细报告
    report_data = []
    for network in suspicious_networks:
        report_data.append({
            'network_id': network['network_id'],
            'company_count': network['size'],
            'risk_score': network['risk_score'],
            'rotation_score': network.get('rotation_score', 0),
            'amount_similarity': network.get('amount_similarity', 0),
            'threshold_ratio': network.get('threshold_ratio', 0),
            'network_density': network.get('network_density', 0),
            'contract_count': network.get('contract_count', 0),
            'total_amount': network.get('total_amount', 0),
            'companies': ', '.join([
                company_names.get(c, c) for c in network['companies'][:5]
            ]) + ('...' if len(network['companies']) > 5 else '')
        })
    
    report_df = pd.DataFrame(report_data)
    report_df = report_df.sort_values('risk_score', ascending=False)
    
    print("\n[3/3] 生成报告...")
    report_df.to_csv(
        'collusion_network_report.csv',
        index=False,
        encoding='utf-8-sig'
    )
    
    print("\n前 5 高风险串通网络：\n")
    for idx, row in report_df.head(5).iterrows():
        print(f"{row['network_id']}:")
        print(f"  公司数量: {row['company_count']}")
        print(f"  风险分数: {row['risk_score']:.4f}")
        print(f"  轮换分数: {row['rotation_score']:.4f}")
        print(f"  金额相似度: {row['amount_similarity']:.4f}")
        print(f"  卡阈值比例: {row['threshold_ratio']:.2%}")
        print(f"  网络密度: {row['network_density']:.4f}")
        print(f"  合同总数: {row['contract_count']}")
        print(f"  涉及金额: ¥{row['total_amount']:,.2f}")
        print(f"  公司列表: {row['companies']}")
        print()
    
    print(f"完整报告已保存至: collusion_network_report.csv")
    
    # 可视化（可选）
    print("\n[可选] 生成网络可视化...")
    visualize_collusion_network(suspicious_networks[0], GRAPH_DATA_DIR)

def visualize_collusion_network(network, graph_data_dir):
    """
    生成串通网络的可视化（HTML）
    """
    import json
    
    # 加载公司信息
    companies_df = pd.read_csv(os.path.join(graph_data_dir, 'nodes_company.csv'))
    company_info = companies_df.set_index('node_id').to_dict('index')
    
    # 构建节点和边数据
    nodes = []
    for comp_id in network['companies']:
        info = company_info.get(comp_id, {})
        nodes.append({
            'id': comp_id,
            'label': info.get('name', comp_id),
            'title': f"{info.get('name', comp_id)}<br>法人: {info.get('legal_person', 'N/A')}"
        })
    
    # 从关联关系构建边（简化版：只显示法人关系）
    edges = []
    legal_person_edges_file = os.path.join(graph_data_dir, 'edges_legal_person.csv')
    legal_person_df = pd.read_csv(legal_person_edges_file)
    
    from collections import defaultdict
    person_to_companies = defaultdict(list)
    
    for _, row in legal_person_df.iterrows():
        if row['to_node'] in network['companies']:
            person_to_companies[row['from_node']].append(row['to_node'])
    
    for person_id, companies in person_to_companies.items():
        if len(companies) >= 2:
            for i in range(len(companies)):
                for j in range(i + 1, len(companies)):
                    edges.append({
                        'from': companies[i],
                        'to': companies[j],
                        'title': f'共享法人: {person_id}'
                    })
    
    # 生成 HTML
    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <title>串通网络可视化 - {network_id}</title>
        <script src="https://cdn.jsdelivr.net/npm/vis-network@9.1.2/dist/vis-network.min.js"></script>
        <style>
            #network {{ width: 100%; height: 800px; border: 1px solid #ddd; }}
            body {{ font-family: Arial, sans-serif; }}
            h1 {{ text-align: center; }}
        </style>
    </head>
    <body>
        <h1>串通网络可视化 - {network_id}</h1>
        <div id="network"></div>
        <script>
            var nodes = new vis.DataSet({nodes_json});
            var edges = new vis.DataSet({edges_json});
            
            var container = document.getElementById('network');
            var data = {{ nodes: nodes, edges: edges }};
            var options = {{
                nodes: {{
                    shape: 'box',
                    color: {{ background: '#FF6B6B', border: '#C92A2A' }},
                    font: {{ color: '#fff' }}
                }},
                edges: {{
                    color: '#999',
                    width: 2
                }},
                physics: {{
                    stabilization: true
                }}
            }};
            
            var network = new vis.Network(container, data, options);
        </script>
    </body>
    </html>
    """
    
    html_content = html_template.format(
        network_id=network['network_id'],
        nodes_json=json.dumps(nodes),
        edges_json=json.dumps(edges)
    )
    
    output_file = f"{network['network_id']}_visualization.html"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"  网络可视化已保存至: {output_file}")

if __name__ == '__main__':
    main()
```

---

## <a name="技术栈"></a>技术栈总览与实施指南

### 5.1 核心技术栈

| 组件 | 技术 | 版本 | 用途 |
|------|------|------|------|
| **图数据库** | Nebula Graph | 3.x | 存储和查询知识图谱 |
| **Python 核心** | Python | 3.12+ | 主要开发语言 |
| **图数据库驱动** | nebula3-python | 3.8.3+ | Nebula Graph Python 客户端 |
| **数据处理** | pandas | latest | 数据清洗和分析 |
| **图算法库** | NetworkX | latest | 图算法实现（社区检测、中心性等） |
| **进度条** | tqdm | latest | 长时间任务进度显示 |
| **Web 框架** | Flask | 3.0.0+ | Web Demo 接口 |
| **可视化** | vis.js / Gephi | - | 图可视化 |

### 5.2 项目结构

```
contract-graph/
├── src/
│   ├── analysis/               # 新增：分析模块
│   │   ├── __init__.py
│   │   ├── fraud_rank.py       # FraudRank 算法
│   │   ├── circular_trade.py   # 循环交易检测
│   │   ├── shell_company.py    # 空壳公司检测
│   │   └── collusion.py        # 串通网络检测
│   ├── nebula_import.py        # 现有：数据导入
│   ├── settings.py             # 现有：配置管理
│   └── web_demo.py             # 现有：Web 演示
├── data/
│   └── graph_data/             # CSV 图数据
├── tests/
│   ├── test_fraud_rank.py      # 新增：测试
│   ├── test_circular_trade.py  # 现有
│   └── ...
├── reports/                     # 新增：分析报告输出目录
├── docs/                        # 文档
└── pyproject.toml              # 依赖管理
```

### 5.3 环境配置

#### 步骤 1: 安装依赖

更新 `pyproject.toml`：

```toml
[project]
name = "contract-graph"
version = "0.2.0"
requires-python = ">=3.12"
dependencies = [
    "nebula3-python>=3.8.3",
    "psycopg2>=2.9.11",
    "python-dotenv>=1.2.1",
    "tqdm>=4.67.1",
    "flask>=3.0.0",
    "pandas>=2.2.0",           # 新增
    "networkx>=3.3",           # 新增
    "numpy>=1.26.0",           # 新增
    "scipy>=1.13.0",           # 新增（可选，用于高级统计）
]
```

安装：
```bash
uv sync
```

#### 步骤 2: Nebula Graph 配置

确保 `src/settings.py` 中的配置正确：

```python
import os
from dotenv import load_dotenv

load_dotenv()

class Settings:
    nebula_config = {
        "host": os.getenv("NEBULA_ADDRESS", "172.18.53.63"),
        "port": int(os.getenv("NEBULA_PORT", 9669)),
        "user": os.getenv("NEBULA_USERNAME", "root"),
        "password": os.getenv("NEBULA_PASSWORD", "nebula"),
        "space": os.getenv("NEBULA_SPACE", "contract_1117"),
    }

settings = Settings()
```

### 5.4 完整实施流程

#### 阶段 1: 数据准备（已完成）
- ✅ CSV 数据生成
- ✅ Nebula Graph Schema 创建
- ✅ 数据导入

#### 阶段 2: 分析模块开发（本文档）

**时间安排**：2-3 周

| 周 | 任务 | 交付物 |
|----|------|--------|
| 第 1 周 | FraudRank + 循环交易检测 | 可运行脚本 + 测试报告 |
| 第 2 周 | 空壳公司 + 串通网络检测 | 可运行脚本 + 测试报告 |
| 第 3 周 | Web API 集成 + 可视化 | REST API + 前端展示 |

#### 阶段 3: 测试与优化

```bash
# 运行单个分析
uv run python src/analysis/fraud_rank.py

# 运行完整分析套件
uv run python src/analysis/run_all_analysis.py

# 运行单元测试
uv run pytest tests/
```

#### 阶段 4: 生产部署

1. **性能优化**：
   - 对大规模图使用 Nebula Graph 的并行查询
   - 缓存中间结果
   - 使用 Redis 存储实时分析结果

2. **定时任务**：
```python
# cron job 配置示例
# 每天凌晨 2 点运行分析
0 2 * * * cd /path/to/contract-graph && uv run python src/analysis/run_all_analysis.py
```

3. **监控告警**：
   - 高风险公司发现时发送邮件/企业微信通知
   - 分析失败时记录日志并告警

### 5.5 进阶技术

#### 5.5.1 图神经网络（GNN）

使用 PyTorch Geometric 进行更复杂的图学习：

```bash
# 安装 PyTorch Geometric
uv add torch torch-geometric
```

```python
# 使用 GraphSAGE 学习节点嵌入
import torch
from torch_geometric.nn import SAGEConv
from torch_geometric.data import Data

class FraudDetectionGNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)
    
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x
```

#### 5.5.2 实时流式分析

使用 Kafka + Flink 进行实时交易监控：

```python
# 监控新交易并实时计算风险分数
from pyflink.datastream import StreamExecutionEnvironment

def detect_fraud_in_stream():
    env = StreamExecutionEnvironment.get_execution_environment()
    
    # 从 Kafka 读取交易流
    txn_stream = env.add_source(...)
    
    # 实时计算 FraudRank
    fraud_alerts = txn_stream.map(lambda txn: calculate_realtime_risk(txn))
    
    # 高风险交易推送告警
    fraud_alerts.filter(lambda x: x['risk'] > 0.8).add_sink(...)
```

### 5.6 相关文档和学习资源

#### Nebula Graph
- 官方文档: https://docs.nebula-graph.com.cn/
- nGQL 语法: https://docs.nebula-graph.com.cn/3.5.0/3.ngql-guide/1.nGQL-overview/
- Python 客户端: https://github.com/vesoft-inc/nebula-python

#### NetworkX
- 官方文档: https://networkx.org/documentation/stable/
- 社区检测算法: https://networkx.org/documentation/stable/reference/algorithms/community.html
- 中心性算法: https://networkx.org/documentation/stable/reference/algorithms/centrality.html

#### 图算法
- 《Graph Algorithms》by Mark Needham
- 《Network Science》by Albert-László Barabási (免费在线): http://networksciencebook.com/

#### 反欺诈技术
- 论文: "FraudRank: A Targeted Approach for Fraud Detection in Large-Scale Networks"
- 论文: "Detecting Anomalous Patterns in Networks Using Community Detection"

---

## 附录：快速启动脚本

### 一键运行所有分析

```python
# src/analysis/run_all_analysis.py

import os
import sys
from datetime import datetime

# 添加父目录到 path
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from analysis.fraud_rank import main as fraud_rank_main
from analysis.circular_trade import main as circular_trade_main
from analysis.shell_company import main as shell_company_main
from analysis.collusion import main as collusion_main

def main():
    start_time = datetime.now()
    
    print("\n" + "=" * 80)
    print("知识图谱高级分析套件")
    print(f"开始时间: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    # 创建报告目录
    reports_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'reports')
    os.makedirs(reports_dir, exist_ok=True)
    
    analyses = [
        ("FraudRank 欺诈风险传导分析", fraud_rank_main),
        ("高级循环交易检测", circular_trade_main),
        ("空壳公司网络识别", shell_company_main),
        ("关联方串通网络分析", collusion_main),
    ]
    
    results = {}
    
    for name, func in analyses:
        print(f"\n{'='*80}")
        print(f"正在执行: {name}")
        print('='*80)
        
        try:
            func()
            results[name] = "✓ 成功"
        except Exception as e:
            print(f"\n错误: {str(e)}")
            results[name] = f"✗ 失败: {str(e)}"
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    # 生成总结报告
    print("\n" + "=" * 80)
    print("分析完成总结")
    print("=" * 80)
    print(f"结束时间: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"总耗时: {duration:.2f} 秒")
    print("\n分析结果:")
    for name, status in results.items():
        print(f"  {status} - {name}")
    print("\n所有报告已保存至 reports/ 目录")
    print("=" * 80 + "\n")

if __name__ == '__main__':
    main()
```

### 使用方法

```bash
# 运行完整分析套件
uv run python src/analysis/run_all_analysis.py

# 运行单个分析
uv run python src/analysis/fraud_rank.py
uv run python src/analysis/circular_trade.py
uv run python src/analysis/shell_company.py
uv run python src/analysis/collusion.py
```

---

**文档结束**

本文档提供了四个高价值知识图谱分析场景的完整实施方案，包括：
1. FraudRank 欺诈风险传导分析
2. 高级循环交易检测（分散-汇聚模式）
3. 空壳公司网络识别
4. 关联方串通网络分析

每个场景都包含：业务背景、算法原理、实施步骤、完整代码、预期输出和业务应用建议。
